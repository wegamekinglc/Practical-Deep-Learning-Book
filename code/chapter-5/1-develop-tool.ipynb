{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToCsmp1pc1wS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/1-develop-tool.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/1-develop-tool.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pknp7a70h6IF"
   },
   "source": [
    "# Chapter 5 - From Novice to Master Predictor: Maximizing Convolutional Neural Network Accuracy\n",
    "\n",
    "\n",
    "We explore strategies to maximize the accuracy that our classifier can achieve, with the help of a range of tools including TensorBoard, What-If Tool, tf-explain, TensorFlow Datasets, AutoKeras, AutoAugment. Along the way, we conduct experiments to develop an intuition of what parameters might or might not work for your AI task. [Read online here.](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch05.html)\n",
    "\n",
    "## Tools\n",
    "\n",
    "In this file, we will develop a tool to experiment with various parameter settings of a model. One can choose amongst different kinds of augmentation techniques, use different datasets available in TensorFlow Datasets, choose to train either from scratch or use finetune from MobileNet or any model of your choice, all in the browser without any framework installs on your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hJduD52zNu0Q",
    "outputId": "406835cb-d9a7-43f5-d522-47f7260b04ef"
   },
   "outputs": [],
   "source": [
    "# Perform all installations\n",
    "!pip install tensorflow-gpu==2.0.0\n",
    "!pip install tensorflow-datasets\n",
    "!pip install tensorwatch\n",
    "\n",
    "# Get TensorBoard to run \n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "4UcsyIiLQr_9",
    "outputId": "407b6de2-003a-4987-a2b5-9d0f54d334b0"
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# tfds makes a lot of progress bars and they take up a lot of screen space, so lets diable them\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZ7ZvJY-r2vY"
   },
   "source": [
    "To make experiments reproducible across runs, we control the amount of randomization possible. Randomization is introduced in initialization of weights of models, randomized shuffling of data. \n",
    "\n",
    "Random number generators can be made reproducible by initializing a seed and thatâ€™s exactly what we will do. Various frameworks have their own ways of setting a random seed, some of which are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kww2jNggsSm9"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QI_ZhgbpxcEE"
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "IMG_H = IMG_W = 224\n",
    "IMG_SIZE = 224\n",
    "LOG_DIR = './log'\n",
    "DEGREES = 10  #for rotation\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rv3IASONQeVt"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Choose the dataset that we want to experiment on. We have tried to build this tool in such a way that it works with all the image datasets available in `TensorFlow Datasets`.\n",
    "\n",
    "To see all available datasets in `TensorFlow Datasets`,  use the following `print` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "eo8WzErmRz9C",
    "outputId": "7f2d7d9f-703c-41c3-a048-9d1b6fa2c32f"
   },
   "outputs": [],
   "source": [
    "# View all available datasets\n",
    "print(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jo_r3u6kQcZ6"
   },
   "outputs": [],
   "source": [
    "# Choose dataset\n",
    "\n",
    "#dataset_name = \"colorectal_histology\"\n",
    "#dataset_name = \"caltech101\"\n",
    "#dataset_name = \"oxford_flowers102\"\n",
    "dataset_name = \"cats_vs_dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9pr4pVkSX2D"
   },
   "source": [
    "### Dataset Preprocessing and Augmenting\n",
    "\n",
    "Let's define some preprocessing and augmentation functions.\n",
    "\n",
    "Note that bicubic resizing functionality is not available in TFDS yet\n",
    "\n",
    "All `tf.image` augmentations defined at https://www.tensorflow.org/api_docs/python/tf/image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAp_67kDN_n0"
   },
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    x = tf.image.resize_with_pad(ds['image'], IMG_SIZE, IMG_SIZE)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = (x / 127.5) - 1\n",
    "    return x, ds['label']\n",
    "\n",
    "\n",
    "def augmentation(image, label):\n",
    "    #image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "    # Random Crop: randomly crops an image and fits to given size\n",
    "    #image = tf.image.random_crop(image,[IMG_SIZE, IMG_SIZE, IMG_CHANNELS])\n",
    "\n",
    "    # Brightness: Adjust brightness by a given max_delta\n",
    "    image = tf.image.random_brightness(image, .1)\n",
    "\n",
    "    # Random Contrast: Add a random contrast to the image\n",
    "    image = tf.image.random_contrast(image, lower=0.0, upper=1.0)\n",
    "\n",
    "    # Flip: Left and right\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    # Rotation: Only 90 degrees is currently supported.\n",
    "    # Not all images still look the same after a 90 degree rotation\n",
    "    # Most images are augmented by a 10-30 degree tilt\n",
    "    #image = tf.keras.preprocessing.image.random_rotation(image,10)\n",
    "\n",
    "    # Finally return the augmented image and label\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iahpaGwaSipp"
   },
   "source": [
    "### Dataset Loading\n",
    "\n",
    "Develop handy functions to load training and validation data.\n",
    "\n",
    "Some of the datasets in `TensorFlow Datasets` do not have a `validation` split. For those datasets we take a small percentage of samples from the `training` set, and treat it as the `validation` set. Splitting the dataset using the `weighted_splits` takes care of randomizing and shuffling data between the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsRP-IrrEPcS"
   },
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name, *split_value):\n",
    "    # see all possible splits in dataset\n",
    "    _, info = tfds.load(dataset_name, with_info=True)\n",
    "    #print(info)\n",
    "    if \"validation\" in info.splits:\n",
    "        # then load train and validation\n",
    "        if len(split_value) == 1:\n",
    "            print(\n",
    "                \"INFO: Splitting train dataset according to splits provided by user\"\n",
    "            )\n",
    "            if \"test\" in info.splits:\n",
    "                print('INFO: Test dataset is available')\n",
    "                all_data = tfds.Split.TEST + tfds.Split.TRAIN\n",
    "                print(\"INFO: Added test data to train data\")\n",
    "                train, info_train = tfds.load(dataset_name,\n",
    "                                              split=all_data,\n",
    "                                              with_info=True)\n",
    "                NUM_CLASSES = info_train.features['label'].num_classes\n",
    "                NUM_EXAMPLES = info_train.splits['train'].num_examples\n",
    "            else:\n",
    "                all_data = tfds.Split.TRAIN\n",
    "            split_train, _ = all_data.subsplit(\n",
    "                weighted=[split_value[0], 100 - split_value[0]])\n",
    "            # Load train with the new split\n",
    "            train, info_train = tfds.load(dataset_name,\n",
    "                                          split=split_train,\n",
    "                                          with_info=True)\n",
    "        else:\n",
    "            #load training dataset the without any user intervention way\n",
    "            print(\"INFO: Loading standard splits for training dataset\")\n",
    "            train, info_train = tfds.load(dataset_name,\n",
    "                                          split=tfds.Split.TRAIN,\n",
    "                                          with_info=True)\n",
    "        # Load validation dataset as is standard\n",
    "        val, info_val = tfds.load(dataset_name,\n",
    "                                  split=tfds.Split.VALIDATION,\n",
    "                                  with_info=True)\n",
    "        NUM_CLASSES = info_train.features['label'].num_classes\n",
    "        NUM_EXAMPLES = info_train.splits['train'].num_examples\n",
    "    else:\n",
    "        # Validation not in default datasets\n",
    "        print(\n",
    "            \"INFO: Defining a 90-10 split between training and validation as no default split exists.\"\n",
    "        )\n",
    "        # Here we have defined how to split the original train dataset into train and val\n",
    "        # Use 90% as train dataset and 10% as validation dataset\n",
    "        train, info_train = tfds.load(dataset_name,\n",
    "                                      split='train[:90%]',\n",
    "                                      with_info=True)\n",
    "        val, info_val = tfds.load(dataset_name,\n",
    "                                  split='train[90%:]',\n",
    "                                  with_info=True)\n",
    "        NUM_CLASSES = info_train.features['label'].num_classes\n",
    "        # The total number of classes in training dataset should either be equal to or more than the total number of classes in validation dataset\n",
    "        assert NUM_CLASSES >= info_val.features['label'].num_classes\n",
    "        NUM_EXAMPLES = info_train.splits['train'].num_examples * 0.9\n",
    "\n",
    "    # Standard processing for training and validation set\n",
    "    IMG_H, IMG_W, IMG_CHANNELS = info_train.features['image'].shape\n",
    "    if IMG_H == None or IMG_H != IMG_SIZE:\n",
    "        IMG_H = IMG_SIZE\n",
    "    if IMG_W == None or IMG_W != IMG_SIZE:\n",
    "        IMG_W = IMG_SIZE\n",
    "    if IMG_CHANNELS == None:\n",
    "        IMG_CHANNELS = 3\n",
    "\n",
    "    # Training specific processing\n",
    "    train = train.map(preprocess).repeat().shuffle(SHUFFLE_BUFFER_SIZE).batch(\n",
    "        BATCH_SIZE)\n",
    "    train = train.map(augmentation)\n",
    "    train = train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Validation specific processing\n",
    "    val = val.map(preprocess).repeat().batch(BATCH_SIZE)\n",
    "    val = val.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train, info_train, val, info_val, IMG_H, IMG_W, IMG_CHANNELS, NUM_CLASSES, NUM_EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rusRxOdWWZhZ"
   },
   "source": [
    "Now that we have defined all our helper functions, let's use them to get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rpIYrRQwg1kx",
    "outputId": "1e005fa5-9ada-42d2-c3c0-5b2aeb141b6c"
   },
   "outputs": [],
   "source": [
    "train, info_train, val, info_val, IMG_H, IMG_W, IMG_CHANNELS, NUM_CLASSES, NUM_EXAMPLES = get_dataset(\n",
    "    dataset_name, 100)\n",
    "\n",
    "print(\"\\n\\nIMG_H, IMG_W\", IMG_H, IMG_W)\n",
    "print(\"IMG_CHANNELS\", IMG_CHANNELS)\n",
    "print(\"NUM_CLASSES\", NUM_CLASSES)\n",
    "print(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "print(\"NUM_EXAMPLES\", NUM_EXAMPLES)\n",
    "print(\"NUM_EPOCHS\", NUM_EPOCHS)\n",
    "\n",
    "# If you want to print even more information on both the splits, uncomment the lines below:\n",
    "#print(info_train)\n",
    "#print(info_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chn8a67jWgn4"
   },
   "source": [
    "Great! \n",
    "\n",
    "## Training\n",
    "\n",
    "Here we decide what kind of training to perform. \n",
    "\n",
    "We have defined a model from scratch as well as a model that performs transfer learning on MobileNet. \n",
    "\n",
    "Depending on what dataset we chose, and how different that dataset is from `ImageNet dataset`, we may learn a lot from the experiments on both kinds of trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZ1lbZR3dy1i"
   },
   "outputs": [],
   "source": [
    "#choose \"scratch\" to train a new model from scratch\n",
    "training_format = \"scratch\"\n",
    "\n",
    "#choose transfer_learning to use finetuning on mobilenet\n",
    "training_format = \"transfer_learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "g0A6MFDBXyQr",
    "outputId": "e673f9a7-d0aa-448b-fbf5-80d52fb25ee7"
   },
   "outputs": [],
   "source": [
    "#Allow TensorBoard callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(LOG_DIR,\n",
    "                                                      histogram_freq=1,\n",
    "                                                      write_graph=True,\n",
    "                                                      write_grads=True,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition for training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mpp4mC6pX2uE"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3),\n",
    "                               activation='relu',\n",
    "                               input_shape=(IMG_SIZE, IMG_SIZE, IMG_CHANNELS)),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(rate=0.3),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.3),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def scratch(train, val, learning_rate):\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy', min_delta=0.0001, patience=5)\n",
    "    csv_logger = CSVLogger('colorectal-scratch-' + 'log.csv',\n",
    "                           append=True,\n",
    "                           separator=';')\n",
    "\n",
    "    model.fit(train,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              steps_per_epoch=int(NUM_EXAMPLES / BATCH_SIZE),\n",
    "              validation_data=val,\n",
    "              validation_steps=1,\n",
    "              validation_freq=1,\n",
    "              callbacks=[tensorboard_callback, earlystop_callback, csv_logger])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Defination for fine-tuning\n",
    "\n",
    "We will be using the MobileNet model for fine-tuning. We can decide how many layers of the model to train by un-freeze the top layers of the model as follows:\n",
    "\n",
    "```\n",
    "#Our unfreeze_percentage variable helps us decide how many layers to unfreeze\n",
    "unfreeze_percentage = 0.75\n",
    "\n",
    "#Initially set all layers to fixed, i.e., not trainable\n",
    "mobile_net.trainable=False\n",
    "  \n",
    "#Find total number of layers in base model \n",
    "num_layers = len(mobile_net.layers)\n",
    "print(\"Total number of layers in MobileNet: \", num_layers)\n",
    " \n",
    "#Set the last few layers to be trainable\n",
    "for layer_index in range(int(num_layers - unfreeze_percentage*num_layers), num_layers):\n",
    "    print(layer_index, mobile_net.layers[layer_index])\n",
    "    \n",
    "    #set the layer to be trainable\n",
    "    mobile_net.layers[layer_index].trainable = True\n",
    "```\n",
    "\n",
    "All you need to do is unfreeze the last few layers in the `mobile_net` model or set them trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WM_eV6RSyIW"
   },
   "outputs": [],
   "source": [
    "def transfer_learn(train, val, unfreeze_percentage, learning_rate):\n",
    "    mobile_net = tf.keras.applications.MobileNet(input_shape=(IMG_SIZE,\n",
    "                                                              IMG_SIZE,\n",
    "                                                              IMG_CHANNELS),\n",
    "                                                 include_top=False)\n",
    "    # Use mobile_net.summary() to view the model\n",
    "    mobile_net.trainable = False\n",
    "    # Unfreeze some of the layers according to the dataset being used\n",
    "    num_layers = len(mobile_net.layers)\n",
    "    for layer_index in range(\n",
    "            int(num_layers - unfreeze_percentage * num_layers), num_layers):\n",
    "        mobile_net.layers[layer_index].trainable = True\n",
    "    model_with_transfer_learning = tf.keras.Sequential([\n",
    "        mobile_net,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ], )\n",
    "    model_with_transfer_learning.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[\"accuracy\"])\n",
    "    model_with_transfer_learning.summary()\n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
    "    csv_logger = CSVLogger('colorectal-transferlearn-' + 'log.csv',\n",
    "                           append=True,\n",
    "                           separator=';')\n",
    "    model_with_transfer_learning.fit(\n",
    "        train,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        steps_per_epoch=int(NUM_EXAMPLES / BATCH_SIZE),\n",
    "        validation_data=val,\n",
    "        validation_steps=1,\n",
    "        validation_freq=1,\n",
    "        callbacks=[tensorboard_callback, earlystop_callback, csv_logger])\n",
    "    return model_with_transfer_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOg-rjnBugSj"
   },
   "source": [
    "Recall that `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout` are applicable only during training. They are turned off when calculating validation loss.\n",
    "\n",
    "Also, remember that training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer.\n",
    "\n",
    "Run the following cells to start the training and then come back up to look at TensorBoard.\n",
    "\n",
    "## TensorBoard\n",
    "\n",
    "Let's focus on what we can learn fom TensorBoard:\n",
    "\n",
    "1. Visualize the training and validation accuracy and loss.\n",
    "2. View the output from each layer by clicking on the `Images` tab. This takes some time to load so grab a coffee!\n",
    "\n",
    "We can edit the size of the results we are looking at, add additional contrast and brightness.\n",
    "\n",
    "3. Visualize the graph of the network that we just trained. \n",
    "4. The `Distributions` tab shows the weight distribution of the weight matrices of each of the layers. This is very useful when quantizing a model. And we will be learning more about this in the later chapters. We can view the histogram of this distribution in the `Histogram` tab.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note: You can ALT+Scroll in and out for zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "mWDKvkWARtQ2",
    "outputId": "b6fffb82-abfa-41a0-ecc1-815e56b7943a"
   },
   "outputs": [],
   "source": [
    "# Start TensorBoard\n",
    "%tensorboard --logdir ./log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "AU5zrBpccru3",
    "outputId": "18f4b088-b998-4e59-d3b5-0053a1c35724"
   },
   "outputs": [],
   "source": [
    "# select the percentage of layers to be trained while using the transfer learning\n",
    "# technique. The selected layers will be close to the output/final layers.\n",
    "unfreeze_percentage = 0\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "if training_format == \"scratch\":\n",
    "    print(\"Training a model from scratch\")\n",
    "    model = scratch(train, val, learning_rate)\n",
    "elif training_format == \"transfer_learning\":\n",
    "    print(\"Fine Tuning the MobileNet model\")\n",
    "    model = transfer_learn(train, val, unfreeze_percentage, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to load it in the What-If tool\n",
    "\n",
    "tf.saved_model.save(model, \"tmp/model/1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded = tf.saved_model.load(\"tmp/model/1\")\n",
    "print(list(loaded.signatures.keys()))  # [\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the directory so that we can download it\n",
    "!zip model.zip  tmp/model/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this in Google Colab,\n",
    "# Go to the content directory and download the trained model\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In our experiment, we were able to modify how many layers of the MobileNet V2 base model we wanted to train. Our training process nudged the weights from a more generic feature set to features associated specifically to the dataset at hand. This is relevant for datasets that are quite different from the ImageNet dataset, or are much smaller.\n",
    "\n",
    "As we learnt in this and the previous chapters, the higher up a layer is, the more specialized it is to the task at hand. The initial layers learn very simple features, like distinguishing an edge, a feature that is common to almost all images. Then, as we proceed to higher layers, features become more specific to the training dataset. \n",
    "\n",
    "Through fine-tuning, we attempted to nudge these higher-layer/specific features to work with a new dataset while still making use of the generic layers."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1-develop-tool",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
